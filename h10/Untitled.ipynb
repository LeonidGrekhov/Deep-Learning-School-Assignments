{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fbf9223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models     import Sequential\n",
    "from keras.layers     import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b979f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "goal_steps = 200\n",
    "score_requirement = -198\n",
    "intial_games = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a39225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_a_random_game_first():\n",
    "    for step_index in range(goal_steps):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(\"Step {}:\".format(step_index))\n",
    "        print(\"action: {}\".format(action))\n",
    "        print(\"observation: {}\".format(observation))\n",
    "        print(\"reward: {}\".format(reward))\n",
    "        print(\"done: {}\".format(done))\n",
    "        print(\"info: {}\".format(info))\n",
    "        if done:\n",
    "            break\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3756c22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "action: 0\n",
      "observation: [-0.42020121 -0.00177578]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 1:\n",
      "action: 2\n",
      "observation: [-0.42174009 -0.00153888]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 2:\n",
      "action: 2\n",
      "observation: [-0.42303108 -0.00129099]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 3:\n",
      "action: 2\n",
      "observation: [-0.42406493 -0.00103386]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 4:\n",
      "action: 2\n",
      "observation: [-0.42483425 -0.00076932]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 5:\n",
      "action: 0\n",
      "observation: [-0.42733351 -0.00249926]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 6:\n",
      "action: 0\n",
      "observation: [-0.43154476 -0.00421125]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 7:\n",
      "action: 1\n",
      "observation: [-0.43643768 -0.00489292]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 8:\n",
      "action: 0\n",
      "observation: [-0.44297688 -0.0065392 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 9:\n",
      "action: 1\n",
      "observation: [-0.45011487 -0.00713799]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 10:\n",
      "action: 0\n",
      "observation: [-0.45879953 -0.00868466]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 11:\n",
      "action: 1\n",
      "observation: [-0.46796714 -0.0091676 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 12:\n",
      "action: 0\n",
      "observation: [-0.47855004 -0.01058291]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 13:\n",
      "action: 2\n",
      "observation: [-0.48846978 -0.00991974]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 14:\n",
      "action: 2\n",
      "observation: [-0.49765251 -0.00918272]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 15:\n",
      "action: 0\n",
      "observation: [-0.50802963 -0.01037712]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 16:\n",
      "action: 2\n",
      "observation: [-0.51752348 -0.00949385]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 17:\n",
      "action: 1\n",
      "observation: [-0.5270629  -0.00953941]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 18:\n",
      "action: 1\n",
      "observation: [-0.53657633 -0.00951343]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 19:\n",
      "action: 0\n",
      "observation: [-0.54699245 -0.01041613]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 20:\n",
      "action: 0\n",
      "observation: [-0.55823327 -0.01124082]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 21:\n",
      "action: 1\n",
      "observation: [-0.56921479 -0.01098152]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 22:\n",
      "action: 1\n",
      "observation: [-0.57985527 -0.01064047]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 23:\n",
      "action: 2\n",
      "observation: [-0.58907581 -0.00922055]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 24:\n",
      "action: 2\n",
      "observation: [-0.59680844 -0.00773262]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 25:\n",
      "action: 2\n",
      "observation: [-0.60299639 -0.00618795]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 26:\n",
      "action: 0\n",
      "observation: [-0.60959448 -0.00659809]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 27:\n",
      "action: 2\n",
      "observation: [-0.61455473 -0.00496025]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 28:\n",
      "action: 0\n",
      "observation: [-0.61984124 -0.00528651]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 29:\n",
      "action: 2\n",
      "observation: [-0.62341592 -0.00357468]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 30:\n",
      "action: 0\n",
      "observation: [-0.62725311 -0.00383719]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 31:\n",
      "action: 1\n",
      "observation: [-0.63032537 -0.00307225]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 32:\n",
      "action: 2\n",
      "observation: [-0.63161078 -0.00128541]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 33:\n",
      "action: 2\n",
      "observation: [-6.31100198e-01  5.10577792e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 34:\n",
      "action: 0\n",
      "observation: [-6.30797264e-01  3.02934571e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 35:\n",
      "action: 1\n",
      "observation: [-0.62970413  0.00109314]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 36:\n",
      "action: 0\n",
      "observation: [-0.62882857  0.00087556]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 37:\n",
      "action: 0\n",
      "observation: [-0.62817684  0.00065173]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 38:\n",
      "action: 1\n",
      "observation: [-0.62675357  0.00142327]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 39:\n",
      "action: 1\n",
      "observation: [-0.62456893  0.00218464]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 40:\n",
      "action: 2\n",
      "observation: [-0.62063855  0.00393039]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 41:\n",
      "action: 1\n",
      "observation: [-0.6159906   0.00464795]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 42:\n",
      "action: 2\n",
      "observation: [-0.60965855  0.00633205]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 43:\n",
      "action: 1\n",
      "observation: [-0.6026882   0.00697035]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 44:\n",
      "action: 0\n",
      "observation: [-0.59613023  0.00655797]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 45:\n",
      "action: 2\n",
      "observation: [-0.58803256  0.00809767]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 46:\n",
      "action: 2\n",
      "observation: [-0.57845463  0.00957792]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 47:\n",
      "action: 1\n",
      "observation: [-0.56846714  0.00998749]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 48:\n",
      "action: 1\n",
      "observation: [-0.55814416  0.01032299]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 49:\n",
      "action: 2\n",
      "observation: [-0.54656255  0.01158161]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 50:\n",
      "action: 2\n",
      "observation: [-0.53380884  0.0127537 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 51:\n",
      "action: 0\n",
      "observation: [-0.52197858  0.01183027]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 52:\n",
      "action: 2\n",
      "observation: [-0.50916046  0.01281812]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 53:\n",
      "action: 2\n",
      "observation: [-0.4954506   0.01370986]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 54:\n",
      "action: 1\n",
      "observation: [-0.48195159  0.013499  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 55:\n",
      "action: 1\n",
      "observation: [-0.46876413  0.01318746]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 56:\n",
      "action: 0\n",
      "observation: [-0.45698608  0.01177805]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 57:\n",
      "action: 0\n",
      "observation: [-0.4467043   0.01028178]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 58:\n",
      "action: 2\n",
      "observation: [-0.43599414  0.01071017]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 59:\n",
      "action: 0\n",
      "observation: [-0.42693346  0.00906067]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 60:\n",
      "action: 2\n",
      "observation: [-0.41758766  0.0093458 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 61:\n",
      "action: 0\n",
      "observation: [-0.41002361  0.00756405]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 62:\n",
      "action: 0\n",
      "observation: [-0.40429498  0.00572863]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 63:\n",
      "action: 2\n",
      "observation: [-0.39844215  0.00585283]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 64:\n",
      "action: 0\n",
      "observation: [-0.3945061   0.00393605]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 65:\n",
      "action: 2\n",
      "observation: [-0.39051422  0.00399188]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 66:\n",
      "action: 1\n",
      "observation: [-0.38749416  0.00302005]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 67:\n",
      "action: 0\n",
      "observation: [-0.38646677  0.0010274 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 68:\n",
      "action: 0\n",
      "observation: [-0.38743909 -0.00097232]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 69:\n",
      "action: 1\n",
      "observation: [-0.38940445 -0.00196536]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 70:\n",
      "action: 0\n",
      "observation: [-0.3933493  -0.00394485]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 71:\n",
      "action: 2\n",
      "observation: [-0.39724635 -0.00389705]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 72:\n",
      "action: 0\n",
      "observation: [-0.40306852 -0.00582216]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 73:\n",
      "action: 2\n",
      "observation: [-0.40877509 -0.00570657]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 74:\n",
      "action: 0\n",
      "observation: [-0.4163259  -0.00755082]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 75:\n",
      "action: 1\n",
      "observation: [-0.42466745 -0.00834155]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 76:\n",
      "action: 2\n",
      "observation: [-0.43274014 -0.00807269]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 77:\n",
      "action: 2\n",
      "observation: [-0.44048586 -0.00774572]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 78:\n",
      "action: 0\n",
      "observation: [-0.44984849 -0.00936263]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 79:\n",
      "action: 2\n",
      "observation: [-0.45875974 -0.00891125]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 80:\n",
      "action: 2\n",
      "observation: [-0.46715423 -0.00839449]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 81:\n",
      "action: 1\n",
      "observation: [-0.47597003 -0.0088158 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 82:\n",
      "action: 2\n",
      "observation: [-0.48414183 -0.0081718 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 83:\n",
      "action: 0\n",
      "observation: [-0.49360887 -0.00946704]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 84:\n",
      "action: 2\n",
      "observation: [-0.50230052 -0.00869166]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 85:\n",
      "action: 2\n",
      "observation: [-0.51015181 -0.00785129]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 86:\n",
      "action: 2\n",
      "observation: [-0.51710392 -0.00695211]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 87:\n",
      "action: 2\n",
      "observation: [-0.52310474 -0.00600082]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 88:\n",
      "action: 1\n",
      "observation: [-0.52910927 -0.00600453]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 89:\n",
      "action: 1\n",
      "observation: [-0.53507247 -0.0059632 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 90:\n",
      "action: 1\n",
      "observation: [-0.54094963 -0.00587716]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 91:\n",
      "action: 2\n",
      "observation: [-0.54569672 -0.00474709]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 92:\n",
      "action: 1\n",
      "observation: [-0.5502782  -0.00458148]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 93:\n",
      "action: 2\n",
      "observation: [-0.55365979 -0.00338159]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 94:\n",
      "action: 0\n",
      "observation: [-0.55781623 -0.00415644]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 95:\n",
      "action: 2\n",
      "observation: [-0.5607165  -0.00290026]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 96:\n",
      "action: 1\n",
      "observation: [-0.56333895 -0.00262245]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 97:\n",
      "action: 2\n",
      "observation: [-0.56466406 -0.00132511]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 98:\n",
      "action: 1\n",
      "observation: [-0.56568196 -0.0010179 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 99:\n",
      "action: 0\n",
      "observation: [-0.56738507 -0.00170311]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 100:\n",
      "action: 1\n",
      "observation: [-0.56876073 -0.00137566]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 101:\n",
      "action: 1\n",
      "observation: [-0.5697987  -0.00103798]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 102:\n",
      "action: 2\n",
      "observation: [-5.69491291e-01  3.07412752e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 103:\n",
      "action: 2\n",
      "observation: [-0.56784077  0.00165052]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 104:\n",
      "action: 2\n",
      "observation: [-0.56485941  0.00298136]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 105:\n",
      "action: 0\n",
      "observation: [-0.56256938  0.00229003]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 106:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 1\n",
      "observation: [-0.55998774  0.00258164]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 107:\n",
      "action: 1\n",
      "observation: [-0.55713372  0.00285402]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 108:\n",
      "action: 1\n",
      "observation: [-0.55402862  0.0031051 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 109:\n",
      "action: 1\n",
      "observation: [-0.55069561  0.00333301]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 110:\n",
      "action: 1\n",
      "observation: [-0.54715959  0.00353601]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 111:\n",
      "action: 2\n",
      "observation: [-0.54244702  0.00471257]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 112:\n",
      "action: 2\n",
      "observation: [-0.53659316  0.00585386]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 113:\n",
      "action: 0\n",
      "observation: [-0.53164187  0.00495129]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 114:\n",
      "action: 0\n",
      "observation: [-0.52763026  0.00401161]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 115:\n",
      "action: 2\n",
      "observation: [-0.52258841  0.00504185]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 116:\n",
      "action: 0\n",
      "observation: [-0.51855414  0.00403427]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 117:\n",
      "action: 1\n",
      "observation: [-0.51455771  0.00399643]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 118:\n",
      "action: 2\n",
      "observation: [-0.50962907  0.00492863]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 119:\n",
      "action: 0\n",
      "observation: [-0.50580518  0.00382389]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 120:\n",
      "action: 2\n",
      "observation: [-0.50111468  0.0046905 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 121:\n",
      "action: 1\n",
      "observation: [-0.49659268  0.004522  ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 122:\n",
      "action: 2\n",
      "observation: [-0.491273    0.00531968]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 123:\n",
      "action: 0\n",
      "observation: [-0.48719539  0.00407761]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 124:\n",
      "action: 2\n",
      "observation: [-0.48239025  0.00480513]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 125:\n",
      "action: 2\n",
      "observation: [-0.4768934   0.00549685]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 126:\n",
      "action: 1\n",
      "observation: [-0.47174569  0.00514771]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 127:\n",
      "action: 2\n",
      "observation: [-0.46598532  0.00576038]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 128:\n",
      "action: 0\n",
      "observation: [-0.46165489  0.00433042]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 129:\n",
      "action: 0\n",
      "observation: [-0.45878638  0.00286851]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 130:\n",
      "action: 2\n",
      "observation: [-0.4554009   0.00338548]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 131:\n",
      "action: 0\n",
      "observation: [-0.45352335  0.00187755]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 132:\n",
      "action: 2\n",
      "observation: [-0.45116749  0.00235585]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 133:\n",
      "action: 1\n",
      "observation: [-0.44935061  0.00181688]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 134:\n",
      "action: 1\n",
      "observation: [-0.448086    0.00126461]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 135:\n",
      "action: 0\n",
      "observation: [-4.48382897e-01 -2.96899867e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 136:\n",
      "action: 0\n",
      "observation: [-0.45023914 -0.00185624]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 137:\n",
      "action: 1\n",
      "observation: [-0.45264115 -0.00240201]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 138:\n",
      "action: 1\n",
      "observation: [-0.45557133 -0.00293018]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 139:\n",
      "action: 1\n",
      "observation: [-0.45900819 -0.00343685]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 140:\n",
      "action: 1\n",
      "observation: [-0.46292645 -0.00391826]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 141:\n",
      "action: 2\n",
      "observation: [-0.46629724 -0.00337079]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 142:\n",
      "action: 0\n",
      "observation: [-0.47109568 -0.00479844]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 143:\n",
      "action: 2\n",
      "observation: [-0.47528626 -0.00419059]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 144:\n",
      "action: 0\n",
      "observation: [-0.48083793 -0.00555166]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 145:\n",
      "action: 2\n",
      "observation: [-0.48570942 -0.00487149]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 146:\n",
      "action: 2\n",
      "observation: [-0.48986447 -0.00415505]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 147:\n",
      "action: 0\n",
      "observation: [-0.49527209 -0.00540762]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 148:\n",
      "action: 2\n",
      "observation: [-0.49989191 -0.00461982]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 149:\n",
      "action: 1\n",
      "observation: [-0.50468938 -0.00479747]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 150:\n",
      "action: 1\n",
      "observation: [-0.5096286  -0.00493922]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 151:\n",
      "action: 2\n",
      "observation: [-0.51367256 -0.00404396]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 152:\n",
      "action: 0\n",
      "observation: [-0.51879096 -0.0051184 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 153:\n",
      "action: 2\n",
      "observation: [-0.52294541 -0.00415445]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 154:\n",
      "action: 1\n",
      "observation: [-0.52710476 -0.00415935]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 155:\n",
      "action: 2\n",
      "observation: [-0.53023782 -0.00313306]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 156:\n",
      "action: 0\n",
      "observation: [-0.5343211  -0.00408327]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 157:\n",
      "action: 0\n",
      "observation: [-0.53932396 -0.00500287]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 158:\n",
      "action: 0\n",
      "observation: [-0.54520893 -0.00588497]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 159:\n",
      "action: 1\n",
      "observation: [-0.55093194 -0.00572301]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 160:\n",
      "action: 2\n",
      "observation: [-0.55545018 -0.00451824]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 161:\n",
      "action: 1\n",
      "observation: [-0.5597299  -0.00427972]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 162:\n",
      "action: 0\n",
      "observation: [-0.56473917 -0.00500926]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 163:\n",
      "action: 1\n",
      "observation: [-0.56944066 -0.00470149]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 164:\n",
      "action: 0\n",
      "observation: [-0.57479942 -0.00535876]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 165:\n",
      "action: 0\n",
      "observation: [-0.58077569 -0.00597627]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 166:\n",
      "action: 2\n",
      "observation: [-0.58532523 -0.00454954]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 167:\n",
      "action: 0\n",
      "observation: [-0.59041446 -0.00508923]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 168:\n",
      "action: 1\n",
      "observation: [-0.59500592 -0.00459146]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 169:\n",
      "action: 1\n",
      "observation: [-0.59906592 -0.00406   ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 170:\n",
      "action: 1\n",
      "observation: [-0.60256474 -0.00349882]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 171:\n",
      "action: 2\n",
      "observation: [-0.60447683 -0.0019121 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 172:\n",
      "action: 0\n",
      "observation: [-0.60678828 -0.00231144]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 173:\n",
      "action: 2\n",
      "observation: [-0.60748226 -0.00069398]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 174:\n",
      "action: 2\n",
      "observation: [-0.60655373  0.00092853]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 175:\n",
      "action: 2\n",
      "observation: [-0.60400944  0.00254429]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 176:\n",
      "action: 2\n",
      "observation: [-0.59986791  0.00414153]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 177:\n",
      "action: 0\n",
      "observation: [-0.59615933  0.00370858]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 178:\n",
      "action: 2\n",
      "observation: [-0.59091084  0.00524849]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 179:\n",
      "action: 0\n",
      "observation: [-0.58616093  0.00474991]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 180:\n",
      "action: 0\n",
      "observation: [-0.58194456  0.00421637]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 181:\n",
      "action: 0\n",
      "observation: [-0.57829282  0.00365174]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 182:\n",
      "action: 1\n",
      "observation: [-0.57423272  0.0040601 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 183:\n",
      "action: 0\n",
      "observation: [-0.57079432  0.0034384 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 184:\n",
      "action: 2\n",
      "observation: [-0.56600313  0.00479118]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 185:\n",
      "action: 1\n",
      "observation: [-0.56089477  0.00510836]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 186:\n",
      "action: 1\n",
      "observation: [-0.55550728  0.0053875 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 187:\n",
      "action: 0\n",
      "observation: [-0.55088083  0.00462645]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 188:\n",
      "action: 0\n",
      "observation: [-0.54705     0.00383083]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 189:\n",
      "action: 1\n",
      "observation: [-0.54304343  0.00400657]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 190:\n",
      "action: 1\n",
      "observation: [-0.5388911   0.00415232]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 191:\n",
      "action: 1\n",
      "observation: [-0.53462413  0.00426698]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 192:\n",
      "action: 1\n",
      "observation: [-0.53027447  0.00434965]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 193:\n",
      "action: 1\n",
      "observation: [-0.52587476  0.00439972]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 194:\n",
      "action: 0\n",
      "observation: [-0.52245797  0.00341679]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 195:\n",
      "action: 2\n",
      "observation: [-0.51804974  0.00440823]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 196:\n",
      "action: 1\n",
      "observation: [-0.51368313  0.00436661]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 197:\n",
      "action: 1\n",
      "observation: [-0.50939087  0.00429226]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 198:\n",
      "action: 1\n",
      "observation: [-0.50520514  0.00418573]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 199:\n",
      "action: 0\n",
      "observation: [-0.5021573   0.00304785]\n",
      "reward: -1.0\n",
      "done: True\n",
      "info: {'TimeLimit.truncated': True}\n"
     ]
    }
   ],
   "source": [
    "play_a_random_game_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f755539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_preparation():\n",
    "    training_data = []\n",
    "    accepted_scores = []\n",
    "    for game_index in range(intial_games):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        previous_observation = []\n",
    "        for step_index in range(goal_steps):\n",
    "            action = random.randrange(0, 3)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(previous_observation) > 0:\n",
    "                game_memory.append([previous_observation, action])\n",
    "                \n",
    "            previous_observation = observation\n",
    "            if observation[0] > -0.2:\n",
    "                reward = 1\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                if data[1] == 1:\n",
    "                    output = [0, 1, 0]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1, 0, 0]\n",
    "                elif data[1] == 2:\n",
    "                    output = [0, 0, 1]\n",
    "                training_data.append([data[0], output])\n",
    "        \n",
    "        env.reset()\n",
    "    \n",
    "    print(accepted_scores)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6597ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-192.0, -196.0, -192.0, -188.0, -168.0, -184.0, -170.0, -178.0, -184.0, -184.0, -186.0, -188.0, -176.0, -170.0, -176.0, -194.0, -188.0, -180.0, -186.0, -174.0, -188.0, -190.0, -176.0, -186.0, -194.0, -180.0, -194.0, -186.0, -178.0, -172.0, -190.0, -146.0, -194.0, -182.0, -184.0, -176.0, -176.0, -178.0, -184.0, -168.0, -164.0, -190.0, -184.0]\n"
     ]
    }
   ],
   "source": [
    "training_data=model_data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06023593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_size, activation='relu'))\n",
    "    model.add(Dense(52, activation='relu'))\n",
    "    model.add(Dense(output_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10223e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_data):\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
    "    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
    "    model = build_model(input_size=len(X[0]), output_size=len(y[0]))\n",
    "    \n",
    "    model.fit(X, y, epochs=5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a6f777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "268/268 [==============================] - 0s 608us/step - loss: 0.2286\n",
      "Epoch 2/5\n",
      "268/268 [==============================] - 0s 873us/step - loss: 0.2218\n",
      "Epoch 3/5\n",
      "268/268 [==============================] - 0s 588us/step - loss: 0.2210\n",
      "Epoch 4/5\n",
      "268/268 [==============================] - 0s 582us/step - loss: 0.2204\n",
      "Epoch 5/5\n",
      "268/268 [==============================] - 0s 604us/step - loss: 0.2199\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ca7fdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0]\n",
      "Average Score: -1.0\n",
      "choice 1:0.15384615384615385  choice 0:0.1806020066889632 choice 2:0.6655518394648829\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(100):\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_obs = []\n",
    "    for step_index in range(goal_steps):\n",
    "        env.render()\n",
    "        if len(prev_obs)==0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])\n",
    "        \n",
    "        choices.append(action)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        game_memory.append([new_observation, action])\n",
    "        score += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.reset()\n",
    "scores.append(score)\n",
    "\n",
    "print(scores)\n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices),choices.count(2)/len(choices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b50fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d03b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
